# -*- coding: utf-8 -*-
"""rekomendasi-tempatwisata-habibfabriarrosyid

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WJeBvZRZbWek_gKsRWSjp5CGveStHwo0

##**Collaborative Filtering**

# Import Library

pada bagian ini akan dilakukan import library yang akan digunakan untuk keseluruhan proyek ini
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

"""- Ini adalah proses load data, mengambil data yang telah diunduh dari platform kaggle
- Link dataset : https://www.kaggle.com/datasets/aprabowo/indonesia-tourism-destination
"""

# Load Data
rate = pd.read_csv('/content/tourism_rating.csv')
point = pd.read_csv('/content/tourism_with_id.csv')
user = pd.read_csv('/content/user.csv')

"""# Praproses"""

rate.columns

point.columns

user.columns

"""- Proses untuk mengecek nilai null pada data rate"""

rate.isnull().sum()

"""- proses pengecekan nilai duplikat pada data rate"""

rate.duplicated().sum()

"""- penghapusan baris yang duplikat pada data rate"""

# Drop duplikat rating
rate = rate.drop_duplicates()

point = point.drop(columns=['Unnamed: 11','Unnamed: 12'])

"""Proses melakukan visualisasi untuk melihat sebaran tempat destinasi berdasarkan rating terbanyak"""

bigten = rate['Place_Id'].value_counts().reset_index()[0:10]
bigten = pd.merge(bigten, point[['Place_Id', 'Place_Name']], how='left', left_on='Place_Id', right_on='Place_Id')
plt.figure(figsize=(10,5))
sns.barplot(x='Place_Id', y='Place_Name', data=bigten)
plt.title('Tempat Destinasi dengan rating terbanyak')
plt.ylabel('Tempat Destinasi')

plt.xlabel('Jumlah Rating')
plt.show()

"""Tahapan melihat visualisasi sebaran tempat destinasi berdasarkan kategori untuk melihat mana yang lebih banyak"""

sns.countplot(y='Category', data=point)
plt.title('Tempat Destinasi Berdasarkan Kategori')
plt.ylabel('Kategori')
plt.xlabel('Tempat Destinasi')
plt.show()

"""Bagian untuk melihat umur pengguna pada data"""

plt.figure(figsize=(10,5))
sns.boxplot(user['Age'])
plt.title('Umur Pengguna')
plt.ylabel('Umur')
plt.xlabel('Pengguna')
plt.show()

"""Bagian untuk melihat sebaran harga masuk destinasi"""

plt.figure(figsize=(10,5))
plt.hist(point['Price'],bins=20)
plt.xlabel('Price')
plt.ylabel("Frequency")
plt.title('Harga Masuk Destinasi')
plt.show()

"""Melihat visualisasi dari persebaran lokasi pengguna yang ada di dalam dataset"""

lokasi = user['Location'].apply(lambda x: x.split(',')[0])
plt.figure(figsize=(10,5))
sns.countplot(y=lokasi)
plt.title('Lokasi Pengguna')
plt.show()

"""Karena basisnya sekarang ingin terfokus pada daerah Yogyakarta saja maka dilakukan filtering untuk mengambil data hanya dari daerah yogyakarta saja"""

# Filter destinasi hanya dari Yogyakarta
point = point[point['City'] == 'Yogyakarta']
rate = pd.merge(rate, point[['Place_Id']], how='right', on='Place_Id')

point.shape

"""Melihat sebaran dari tempat destinasi dengan rating terbanyak di jogja"""

jogja = rate['Place_Id'].value_counts().reset_index()[0:10]
jogja = pd.merge(jogja, point[['Place_Id', 'Place_Name']], how='left', left_on='Place_Id', right_on='Place_Id')
plt.figure(figsize=(10,5))
sns.barplot(x='Place_Id', y='Place_Name', data=jogja)
plt.title('Tempat Destinasi dengan rating terbanyak di Jogja')
plt.ylabel('Tempat Destinasi')

plt.xlabel('Jumlah Rating')
plt.show()

"""Melihat visualisasi tempat destinasi di jogja berdasarkan kategori"""

sns.countplot(y='Category', data=point)
plt.title('Tempat Destinasi di Jogja Berdasarkan Kategori')
plt.ylabel('Kategori')
plt.xlabel('Tempat Destinasi')
plt.show()

"""Melihat visualisasi dari harga masuk destinasi di jogja"""

plt.figure(figsize=(10,5))
plt.hist(point['Price'],bins=20)
plt.xlabel('Price')
plt.ylabel("Frequency")
plt.title('Harga Masuk Destinasi di Jogja')
plt.show()

"""dilakukan proses mengubah kolom User_Id dan Place_Id dari data asli menjadi bentuk numerik (integer) yang dapat diproses oleh model machine learning atau deep learning untuk keperluan sistem rekomendasi."""

# Membuat mapping ID
user_ids = rate['User_Id'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for x, i in user_to_user_encoded.items()}

place_ids = rate['Place_Id'].unique().tolist()
place_to_place_encoded = {x: i for i, x in enumerate(place_ids)}
place_encoded_to_place = {i: x for x, i in place_to_place_encoded.items()}

"""dilakukan proses menambahkan dua kolom baru pada DataFrame rate, yaitu kolom user dan place, yang berisi hasil encoding dari User_Id dan Place_Id menggunakan dictionary mapping yang telah dibuat sebelumnya (user_to_user_encoded dan place_to_place_encoded)"""

# Tambahkan kolom encoded
rate['user'] = rate['User_Id'].map(user_to_user_encoded)
rate['place'] = rate['Place_Id'].map(place_to_place_encoded)

num_users = len(user_ids)
num_place = len(place_ids)

"""dilakukan normalisasi terhadap kolom Place_Ratings dalam DataFrame rate ke dalam rentang 0 hingga 1 agar skala data lebih seragam"""

# Normalisasi rating ke 0-1
min_rating = min(rate['Place_Ratings'])
max_rating = max(rate['Place_Ratings'])
rate['normalized_rating'] = rate['Place_Ratings'].apply(lambda x: (x - min_rating) / (max_rating - min_rating))

"""proses pembagian data latih dan data uji"""

# Siapkan data training
x = rate[['user', 'place']].values
y = rate['normalized_rating'].values

# Split train/val
from sklearn.model_selection import train_test_split
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)

"""# Modelling

Proses melatih sebuah model rekomendasi berbasis neural network menggunakan TensorFlow dan Keras. Model RecommenderNet adalah subclass dari tf.keras.Model yang memanfaatkan layer embedding untuk merepresentasikan pengguna dan tempat (user dan place) dalam vektor berdimensi rendah (embedding_size).
"""

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_places, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.user_embedding = layers.Embedding(num_users, embedding_size, embeddings_initializer='he_normal',
                                               embeddings_regularizer=keras.regularizers.l2(1e-6))
        self.user_bias = layers.Embedding(num_users, 1)
        self.places_embedding = layers.Embedding(num_places, embedding_size, embeddings_initializer='he_normal',
                                                 embeddings_regularizer=keras.regularizers.l2(1e-6))
        self.places_bias = layers.Embedding(num_places, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        places_vector = self.places_embedding(inputs[:, 1])
        places_bias = self.places_bias(inputs[:, 1])
        dot_user_places = tf.reduce_sum(user_vector * places_vector, axis=1, keepdims=True)
        x = dot_user_places + user_bias + places_bias
        return tf.nn.sigmoid(x)

model = RecommenderNet(num_users, num_place, 50)
model.compile(loss=tf.keras.losses.BinaryCrossentropy(),
              optimizer=keras.optimizers.Adagrad(learning_rate=0.0003),
              metrics=[tf.keras.metrics.RootMeanSquaredError()])

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if logs.get('val_root_mean_squared_error') < 0.25:
            print('\nRoot metrics validasi sudah sesuai harapan')
            self.model.stop_training = True

history = model.fit(
    x=x_train,
    y=y_train,
    epochs=50,
    validation_data=(x_val, y_val),
    callbacks=[myCallback()]
)

"""# Evaluasi

Evaluasi dilakukan dengan melihat hasil dari nilai rmse yang dihasilkan dari data pelatihan disertakan visualisasi
"""

import matplotlib.pyplot as plt

# Misalnya ini adalah history dari model setelah training
# (pastikan 'history' adalah hasil dari model.fit(...))
rmse = history.history['root_mean_squared_error']
val_rmse = history.history['val_root_mean_squared_error']

plt.figure(figsize=(10, 6))
plt.plot(rmse, label='Training RMSE', color='orange')
plt.plot(val_rmse, label='Validation RMSE', color='red', linestyle='--')

plt.title('Training vs Validation RMSE per Epoch')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

!pip install tabulate

"""Proses uji coba model apakah bisa melakukan rekomendasi"""

from tabulate import tabulate
import numpy as np

# Input user ID secara manual
try:
    user_id = int(input("Masukkan User ID: "))
except ValueError:
    print("User ID harus berupa angka.")
    exit()

# Proses rekomendasi
encoded_user_id = user_to_user_encoded.get(user_id)

# Validasi jika user ID tidak ditemukan
if encoded_user_id is None:
    print(f"User ID {user_id} tidak ditemukan dalam data.")
    exit()

place_visited_by_user = rate[rate['User_Id'] == user_id]
place_not_visited = list(set(place_ids) - set(place_visited_by_user['Place_Id'].values))
place_not_visited_encoded = [place_to_place_encoded.get(x) for x in place_not_visited]

user_place_array = np.array([[encoded_user_id, place] for place in place_not_visited_encoded])
ratings = model.predict(user_place_array).flatten()
top_ratings_indices = ratings.argsort()[-7:][::-1]
recommended_place_ids = [place_encoded_to_place[place_not_visited_encoded[x]] for x in top_ratings_indices]

print('=' * 50)
print(f"Rekomendasi Tempat Wisata untuk User {user_id}")
print('=' * 50)

# Top 5 tempat yang pernah disukai
top_place_user = place_visited_by_user.sort_values(
    by='Place_Ratings', ascending=False).head(5)['Place_Id'].values
place_df_rows = point[point['Place_Id'].isin(top_place_user)]

top_visited_table = [
    [row['Place_Name'], row['Category'], row['Rating'], row['Price']]
    for _, row in place_df_rows.iterrows()
]
print("\nTempat yang Pernah Disukai:")
print(tabulate(top_visited_table, headers=["Nama Tempat", "Kategori", "Rating", "Harga"], tablefmt="fancy_grid"))

# Rekomendasi 7 tempat
recommended_place = point[point['Place_Id'].isin(recommended_place_ids)]
recommended_table = [
    [i + 1, row['Place_Name'], row['Category'], row['Rating'], row['Price']]
    for i, (_, row) in enumerate(recommended_place.iterrows())
]
print("\nRekomendasi Tempat:")
print(tabulate(recommended_table, headers=["#", "Nama Tempat", "Kategori", "Rating", "Harga"], tablefmt="fancy_grid"))

"""# **Content Based Filtering**

ekstraksi fitur konten
"""

indices = pd.Series(range(len(point)), index=point['Place_Id'])

"""melihat ukuran dari cosine, baris, dan key sampel"""

print(f"Ukuran cosine_sim: {cosine_sim.shape}")
print(f"Jumlah baris point: {len(point)}")
print(f"Indices keys example: {list(indices.keys())[:10]}")
print(f"indices[{179}]: {indices.get(179)}")

"""melihat key dari data yang dapat diujikan"""

print(point['Place_Id'].unique())

"""melihat isi data"""

point

!pip install Sastrawi

"""proses ekstraksi fitur dan mengambil skor similarity dari content based filtering"""

import re
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd

# Fungsi preprocessing teks
def preprocess_text(text):
    text = text.lower()  # Ubah ke huruf kecil
    text = re.sub(r'[^\w\s]', '', text)  # Hapus tanda baca
    return text

# Gabungkan Description dan Category
point['combined_features'] = point['Description'].fillna('') + ' ' + point['Category'].fillna('')
descriptions = point['combined_features'].apply(preprocess_text)

# Gunakan stop words bahasa Indonesia
stopword_factory = StopWordRemoverFactory()
stop_words = stopword_factory.get_stop_words()
tfidf = TfidfVectorizer(stop_words=stop_words)
tfidf_matrix = tfidf.fit_transform(descriptions)
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

"""membuat fungsi rekomendasi yang mengambil id dan mengurutkan sesuai hasil skor similaritasnya dari tertinggi ke terendah dan hanya mengambil 5 sampel saja"""

def get_content_recommendations(place_id, cosine_sim=cosine_sim, top_n=5):
    """
    Mengembalikan top_n destinasi paling relevan berdasarkan kesamaan Cosine Similarity.

    Parameters:
    - place_id: ID destinasi acuan
    - cosine_sim: Matriks Cosine Similarity
    - top_n: Jumlah rekomendasi yang diinginkan (default 5)

    Returns:
    - DataFrame berisi top_n destinasi relevan, diurutkan berdasarkan skor kesamaan
    """
    if place_id not in indices:
        raise ValueError(f"Place_ID {place_id} tidak ditemukan.")

    idx = indices[place_id]
    # Ambil semua skor kesamaan untuk destinasi ini
    sim_scores = list(enumerate(cosine_sim[idx]))
    # Urutkan berdasarkan skor kesamaan (descending)
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    # Ambil top_n destinasi (kecuali diri sendiri)
    sim_scores = sim_scores[1:top_n+1]

    # Ambil indeks destinasi yang relevan
    place_indices = [i[0] for i in sim_scores]

    # Kembalikan DataFrame dengan kolom relevan
    result = point.iloc[place_indices][['Place_Id', 'Place_Name', 'Category', 'Rating', 'Price']].copy()
    # Tambahkan kolom skor kesamaan
    result['Similarity_Score'] = [score[1] for score in sim_scores]

    return result

"""mengujikan ke salah satu id"""

# Contoh penggunaan
place_id = 210  # Misalnya, Pantai Congot
recommendations = get_content_recommendations(place_id)
print(f"Rekomendasi untuk Place_Id={place_id} ({point[point['Place_Id'] == place_id]['Place_Name'].iloc[0]}):")
print(recommendations.to_string(index=False))

"""mengujikan ke id yang lain"""

# Contoh penggunaan
place_id = 179  # Misalnya, Pantai Congot
recommendations = get_content_recommendations(place_id)
print(f"Rekomendasi untuk Place_Id={place_id} ({point[point['Place_Id'] == place_id]['Place_Name'].iloc[0]}):")
print(recommendations.to_string(index=False))

"""Mengevaluasi bagaimana skor recall yang dihasilkan"""

# Fungsi calculate_recall_at_5 yang dimodifikasi
def calculate_recall_at_5(place_id, recommendations):
    # Ambil hanya 5 rekomendasi teratas
    top_5_recommendations = recommendations.head(5)
    # Hitung jumlah item relevan dalam 5 rekomendasi teratas
    relevant_in_recommendations = top_5_recommendations[
        (top_5_recommendations['Category'] == point[point['Place_Id'] == place_id]['Category'].iloc[0]) &
        (top_5_recommendations['Rating'] >= 4.0)
    ].shape[0]
    # Recall@5 = proporsi item relevan dalam 5 rekomendasi
    recall = relevant_in_recommendations / 5
    return recall, relevant_in_recommendations

# Daftar Place_Id untuk evaluasi
place_ids_to_evaluate = [210, 179]  # Contoh: Pantai Congot dan Taman Sungai Mudal
recall_results = []

for place_id in place_ids_to_evaluate:
    # Dapatkan rekomendasi
    recommendations = get_content_recommendations(place_id)

    # Dapatkan kategori dan rating destinasi acuan
    acuan = point[point['Place_Id'] == place_id][['Place_Name', 'Category', 'Rating']].iloc[0]

    # Hitung Recall@5
    recall, relevant_in_recommendations = calculate_recall_at_5(place_id, recommendations)

    # Simpan hasil
    recall_results.append({
        'Place_Id': place_id,
        'Place_Name': acuan['Place_Name'],
        'Category': acuan['Category'],
        'Recall@5': recall,
        'Relevant_in_Recommendations': relevant_in_recommendations
    })

    # Cetak detail
    print(f"\nEvaluasi untuk Place_Id={place_id} ({acuan['Place_Name']}, Kategori: {acuan['Category']})")
    print("5 Rekomendasi Teratas:")
    print(recommendations.head(5)[['Place_Name', 'Category', 'Rating', 'Similarity_Score']].to_string(index=False))
    print(f"\nItem relevan dalam 5 rekomendasi teratas: {relevant_in_recommendations}")
    print(f"Recall@5: {recall:.2f}")

# Cetak rata-rata Recall@5
average_recall = sum([r['Recall@5'] for r in recall_results]) / len(recall_results)
print(f"\nRata-rata Recall@5: {average_recall:.2f}")

# Cetak ringkasan hasil
print("\nRingkasan Hasil Recall@5:")
for result in recall_results:
    print(f"Place_Id={result['Place_Id']} ({result['Place_Name']}): Recall@5={result['Recall@5']:.2f} "
          f"({result['Relevant_in_Recommendations']}/5)")

