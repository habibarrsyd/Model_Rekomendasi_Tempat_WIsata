# -*- coding: utf-8 -*-
"""rekomendasi-tempatwisata-habibfabriarrosyid

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WJeBvZRZbWek_gKsRWSjp5CGveStHwo0

##**Collaborative Filtering**

# Import Library

pada bagian ini akan dilakukan import library yang akan digunakan untuk keseluruhan proyek ini
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

"""- Ini adalah proses load data, mengambil data yang telah diunduh dari platform kaggle
- Link dataset : https://www.kaggle.com/datasets/aprabowo/indonesia-tourism-destination
"""

# Load Data
rate = pd.read_csv('/content/tourism_rating.csv')
point = pd.read_csv('/content/tourism_with_id.csv')
user = pd.read_csv('/content/user.csv')

"""# Praproses

- Proses untuk mengecek nilai null pada data rate
"""

rate.isnull().sum()

"""- proses pengecekan nilai duplikat pada data rate"""

rate.duplicated().sum()

"""- penghapusan baris yang duplikat pada data rate"""

# Drop duplikat rating
rate = rate.drop_duplicates()

point = point.drop(columns=['Unnamed: 11','Unnamed: 12'])

"""Proses melakukan visualisasi untuk melihat sebaran tempat destinasi berdasarkan rating terbanyak"""

bigten = rate['Place_Id'].value_counts().reset_index()[0:10]
bigten = pd.merge(bigten, point[['Place_Id', 'Place_Name']], how='left', left_on='Place_Id', right_on='Place_Id')
plt.figure(figsize=(10,5))
sns.barplot(x='Place_Id', y='Place_Name', data=bigten)
plt.title('Tempat Destinasi dengan rating terbanyak')
plt.ylabel('Tempat Destinasi')

plt.xlabel('Jumlah Rating')
plt.show()

"""Tahapan melihat visualisasi sebaran tempat destinasi berdasarkan kategori untuk melihat mana yang lebih banyak"""

sns.countplot(y='Category', data=point)
plt.title('Tempat Destinasi Berdasarkan Kategori')
plt.ylabel('Kategori')
plt.xlabel('Tempat Destinasi')
plt.show()

"""Bagian untuk melihat umur pengguna pada data"""

plt.figure(figsize=(10,5))
sns.boxplot(user['Age'])
plt.title('Umur Pengguna')
plt.ylabel('Umur')
plt.xlabel('Pengguna')
plt.show()

"""Bagian untuk melihat sebaran harga masuk destinasi"""

plt.figure(figsize=(10,5))
plt.hist(point['Price'],bins=20)
plt.xlabel('Price')
plt.ylabel("Frequency")
plt.title('Harga Masuk Destinasi')
plt.show()

"""Melihat visualisasi dari persebaran lokasi pengguna yang ada di dalam dataset"""

lokasi = user['Location'].apply(lambda x: x.split(',')[0])
plt.figure(figsize=(10,5))
sns.countplot(y=lokasi)
plt.title('Lokasi Pengguna')
plt.show()

"""Karena basisnya sekarang ingin terfokus pada daerah Yogyakarta saja maka dilakukan filtering untuk mengambil data hanya dari daerah yogyakarta saja"""

# Filter destinasi hanya dari Yogyakarta
point = point[point['City'] == 'Yogyakarta']
rate = pd.merge(rate, point[['Place_Id']], how='right', on='Place_Id')

"""Melihat sebaran dari tempat destinasi dengan rating terbanyak di jogja"""

jogja = rate['Place_Id'].value_counts().reset_index()[0:10]
jogja = pd.merge(jogja, point[['Place_Id', 'Place_Name']], how='left', left_on='Place_Id', right_on='Place_Id')
plt.figure(figsize=(10,5))
sns.barplot(x='Place_Id', y='Place_Name', data=jogja)
plt.title('Tempat Destinasi dengan rating terbanyak di Jogja')
plt.ylabel('Tempat Destinasi')

plt.xlabel('Jumlah Rating')
plt.show()

"""Melihat visualisasi tempat destinasi di jogja berdasarkan kategori"""

sns.countplot(y='Category', data=point)
plt.title('Tempat Destinasi di Jogja Berdasarkan Kategori')
plt.ylabel('Kategori')
plt.xlabel('Tempat Destinasi')
plt.show()

"""Melihat visualisasi dari harga masuk destinasi di jogja"""

plt.figure(figsize=(10,5))
plt.hist(point['Price'],bins=20)
plt.xlabel('Price')
plt.ylabel("Frequency")
plt.title('Harga Masuk Destinasi di Jogja')
plt.show()

"""dilakukan proses mengubah kolom User_Id dan Place_Id dari data asli menjadi bentuk numerik (integer) yang dapat diproses oleh model machine learning atau deep learning untuk keperluan sistem rekomendasi."""

# Membuat mapping ID
user_ids = rate['User_Id'].unique().tolist()
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for x, i in user_to_user_encoded.items()}

place_ids = rate['Place_Id'].unique().tolist()
place_to_place_encoded = {x: i for i, x in enumerate(place_ids)}
place_encoded_to_place = {i: x for x, i in place_to_place_encoded.items()}

"""dilakukan proses menambahkan dua kolom baru pada DataFrame rate, yaitu kolom user dan place, yang berisi hasil encoding dari User_Id dan Place_Id menggunakan dictionary mapping yang telah dibuat sebelumnya (user_to_user_encoded dan place_to_place_encoded)"""

# Tambahkan kolom encoded
rate['user'] = rate['User_Id'].map(user_to_user_encoded)
rate['place'] = rate['Place_Id'].map(place_to_place_encoded)

num_users = len(user_ids)
num_place = len(place_ids)

"""dilakukan normalisasi terhadap kolom Place_Ratings dalam DataFrame rate ke dalam rentang 0 hingga 1 agar skala data lebih seragam"""

# Normalisasi rating ke 0-1
min_rating = min(rate['Place_Ratings'])
max_rating = max(rate['Place_Ratings'])
rate['normalized_rating'] = rate['Place_Ratings'].apply(lambda x: (x - min_rating) / (max_rating - min_rating))

"""proses pembagian data latih dan data uji"""

# Siapkan data training
x = rate[['user', 'place']].values
y = rate['normalized_rating'].values

# Split train/val
from sklearn.model_selection import train_test_split
x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)

"""# Modelling

Proses melatih sebuah model rekomendasi berbasis neural network menggunakan TensorFlow dan Keras. Model RecommenderNet adalah subclass dari tf.keras.Model yang memanfaatkan layer embedding untuk merepresentasikan pengguna dan tempat (user dan place) dalam vektor berdimensi rendah (embedding_size).
"""

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_users, num_places, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.user_embedding = layers.Embedding(num_users, embedding_size, embeddings_initializer='he_normal',
                                               embeddings_regularizer=keras.regularizers.l2(1e-6))
        self.user_bias = layers.Embedding(num_users, 1)
        self.places_embedding = layers.Embedding(num_places, embedding_size, embeddings_initializer='he_normal',
                                                 embeddings_regularizer=keras.regularizers.l2(1e-6))
        self.places_bias = layers.Embedding(num_places, 1)

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        places_vector = self.places_embedding(inputs[:, 1])
        places_bias = self.places_bias(inputs[:, 1])
        dot_user_places = tf.reduce_sum(user_vector * places_vector, axis=1, keepdims=True)
        x = dot_user_places + user_bias + places_bias
        return tf.nn.sigmoid(x)

model = RecommenderNet(num_users, num_place, 50)
model.compile(loss=tf.keras.losses.BinaryCrossentropy(),
              optimizer=keras.optimizers.Adagrad(learning_rate=0.0003),
              metrics=[tf.keras.metrics.RootMeanSquaredError()])

class myCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs={}):
        if logs.get('val_root_mean_squared_error') < 0.25:
            print('\nRoot metrics validasi sudah sesuai harapan')
            self.model.stop_training = True

history = model.fit(
    x=x_train,
    y=y_train,
    epochs=50,
    validation_data=(x_val, y_val),
    callbacks=[myCallback()]
)

"""# Evaluasi

Evaluasi dilakukan dengan melihat hasil dari nilai rmse yang dihasilkan dari data pelatihan disertakan visualisasi
"""

import matplotlib.pyplot as plt

# Misalnya ini adalah history dari model setelah training
# (pastikan 'history' adalah hasil dari model.fit(...))
rmse = history.history['root_mean_squared_error']
val_rmse = history.history['val_root_mean_squared_error']

plt.figure(figsize=(10, 6))
plt.plot(rmse, label='Training RMSE', color='orange')
plt.plot(val_rmse, label='Validation RMSE', color='red', linestyle='--')

plt.title('Training vs Validation RMSE per Epoch')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

!pip install tabulate

"""Proses uji coba model apakah bisa melakukan rekomendasi"""

from tabulate import tabulate
import numpy as np

# Input user ID secara manual
try:
    user_id = int(input("Masukkan User ID: "))
except ValueError:
    print("User ID harus berupa angka.")
    exit()

# Proses rekomendasi
encoded_user_id = user_to_user_encoded.get(user_id)

# Validasi jika user ID tidak ditemukan
if encoded_user_id is None:
    print(f"User ID {user_id} tidak ditemukan dalam data.")
    exit()

place_visited_by_user = rate[rate['User_Id'] == user_id]
place_not_visited = list(set(place_ids) - set(place_visited_by_user['Place_Id'].values))
place_not_visited_encoded = [place_to_place_encoded.get(x) for x in place_not_visited]

user_place_array = np.array([[encoded_user_id, place] for place in place_not_visited_encoded])
ratings = model.predict(user_place_array).flatten()
top_ratings_indices = ratings.argsort()[-7:][::-1]
recommended_place_ids = [place_encoded_to_place[place_not_visited_encoded[x]] for x in top_ratings_indices]

print('=' * 50)
print(f"Rekomendasi Tempat Wisata untuk User {user_id}")
print('=' * 50)

# Top 5 tempat yang pernah disukai
top_place_user = place_visited_by_user.sort_values(
    by='Place_Ratings', ascending=False).head(5)['Place_Id'].values
place_df_rows = point[point['Place_Id'].isin(top_place_user)]

top_visited_table = [
    [row['Place_Name'], row['Category'], row['Rating'], row['Price']]
    for _, row in place_df_rows.iterrows()
]
print("\nTempat yang Pernah Disukai:")
print(tabulate(top_visited_table, headers=["Nama Tempat", "Kategori", "Rating", "Harga"], tablefmt="fancy_grid"))

# Rekomendasi 7 tempat
recommended_place = point[point['Place_Id'].isin(recommended_place_ids)]
recommended_table = [
    [i + 1, row['Place_Name'], row['Category'], row['Rating'], row['Price']]
    for i, (_, row) in enumerate(recommended_place.iterrows())
]
print("\nRekomendasi Tempat:")
print(tabulate(recommended_table, headers=["#", "Nama Tempat", "Kategori", "Rating", "Harga"], tablefmt="fancy_grid"))

"""# **Content Based Filtering**

ekstraksi fitur konten
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Misal kolom fitur text untuk tiap tempat ada di 'Description' atau 'Features'
descriptions = point['Description'].fillna('')  # sesuaikan kolom yang dipakai

tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf.fit_transform(descriptions)

cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# buat indices sesuai semua place_id
indices = pd.Series(range(len(point)), index=point['Place_Id'])

"""membuat fungsi rekomendasi berdasarkan kemiripan konten"""

def get_content_recommendations(place_id, cosine_sim=cosine_sim, top_n=5):
    idx = indices[place_id]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:top_n+1]  # ambil 5 tempat mirip (selain dirinya sendiri)
    place_indices = [i[0] for i in sim_scores]
    return point.iloc[place_indices]

"""melihat ukuran dari cosine, baris, dan key sampel"""

print(f"Ukuran cosine_sim: {cosine_sim.shape}")
print(f"Jumlah baris point: {len(point)}")
print(f"Indices keys example: {list(indices.keys())[:10]}")
print(f"indices[{179}]: {indices.get(179)}")

"""melihat key dari data yang dapat diujikan"""

print(point['Place_Id'].unique())

"""melihat isi data"""

point

"""melihat hasil rekomendasi apakah relevan dengan isi data yang ada atau tidak"""

get_content_recommendations(210)